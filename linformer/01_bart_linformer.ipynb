{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30aa102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pankaj Deb Roy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import transformers\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer,BartConfig,BartModel\n",
    "import math\n",
    "from typing import Union,Tuple\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70edbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bart.modeling_bart import BartScaledWordEmbedding,BartLearnedPositionalEmbedding,BartPreTrainedModel,BartEncoderLayer,BartAttention,BartEncoder\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa,_prepare_4d_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33765206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "bsz=8\n",
    "epoch=10\n",
    "adam_eps=1e-5\n",
    "weight_decay=0.01\n",
    "learning_rate=1e-5\n",
    "warmup=50\n",
    "save_step=1000\n",
    "save_pth='bart_distillition'\n",
    "save_model_name='cnn_bart_encoder_'\n",
    "dataset_pth='./cnn_dataset/'\n",
    "val_length=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66be9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinformerAttention(BartAttention):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "        is_causal: bool = False,\n",
    "        config: Optional[BartConfig] = None,\n",
    "        linformer_dim:Optional[int]=512,\n",
    "    ):\n",
    "        super().__init__(embed_dim=embed_dim,num_heads=num_heads,dropout=dropout,is_decoder=is_decoder,bias=bias,is_causal=is_causal,config=config)\n",
    "\n",
    "        self.kv_former = nn.Linear(self.config.max_position_embeddings,linformer_dim,bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "        # the provided `key_value_states` to support prefix tuning\n",
    "\n",
    "\n",
    "        hidden_states = self.kv_former(hidden_states.transpose(-1,-2)).transpose(-1,-2)\n",
    "\n",
    "        if (\n",
    "            is_cross_attention\n",
    "            and past_key_value is not None\n",
    "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "        ):\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.reshape(*proj_shape)\n",
    "        value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "        \n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned across GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value\n",
    "class LinformerEncoderLayer(BartEncoderLayer):\n",
    "    def __init__(self, config: BartConfig):\n",
    "        super().__init__(config=config)\n",
    "        self.embed_dim = config.d_model\n",
    "\n",
    "        self.self_attn = LinformerAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=config.encoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            config=config,\n",
    "            linformer_dim=512,\n",
    "        )\n",
    "class LinformerEncoder(BartPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
    "    [`BartEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "        embed_tokens (nn.Embedding): output embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.encoder_layerdrop\n",
    "\n",
    "        embed_dim = config.d_model\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.max_source_positions = config.max_position_embeddings\n",
    "        embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
    "\n",
    "        self.embed_tokens = BartScaledWordEmbedding(\n",
    "            config.vocab_size, embed_dim, self.padding_idx, embed_scale=embed_scale\n",
    "        )\n",
    "\n",
    "        if embed_tokens is not None:\n",
    "            self.embed_tokens.weight = embed_tokens.weight\n",
    "\n",
    "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
    "            config.max_position_embeddings,\n",
    "            embed_dim,\n",
    "        )\n",
    "        self.layers = nn.ModuleList([LinformerEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n",
    "        self._use_sdpa = config._attn_implementation == \"sdpa\"\n",
    "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        distil_layer:Optional[int]=-1,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input = input_ids\n",
    "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input = inputs_embeds[:, :, -1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "            embed_pos = self.embed_positions(input)\n",
    "            embed_pos = embed_pos.to(inputs_embeds.device)\n",
    "\n",
    "            hidden_states = inputs_embeds + embed_pos\n",
    "            hidden_states = self.layernorm_embedding(hidden_states)\n",
    "        else: # Pankaj This is for distillition training, as input_embedding is already having positional encoding in it.\n",
    "            hidden_states=inputs_embeds\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            if self._use_flash_attention_2:\n",
    "                attention_mask = attention_mask if 0 in attention_mask else None\n",
    "            elif self._use_sdpa and head_mask is None and not output_attentions:\n",
    "                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n",
    "                # the manual implementation that requires a 4D causal mask in all cases.\n",
    "                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n",
    "            else:\n",
    "                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            if head_mask.size()[0] != (len(self.layers)):\n",
    "                raise ValueError(\n",
    "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    encoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    (head_mask[idx] if head_mask is not None else None),\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = encoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if distil_layer==idx:\n",
    "                return hidden_states\n",
    "                \n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )\n",
    "    \n",
    "class BartDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset,tokenizer,config):\n",
    "        self.dataset=dataset\n",
    "        self.tokenizer=tokenizer\n",
    "        self.config=config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def add_prefix(self,text):\n",
    "        prefix = \"SUMMARIZE NEWS : \"\n",
    "        return prefix+text\n",
    "    def __getitem__(self,idx):\n",
    "        data = self.dataset.iloc[idx]\n",
    "        text,summary = data.article,data.highlights\n",
    "        text = self.add_prefix(text)\n",
    "        inputs = self.tokenizer(text,max_length=self.config.max_position_embeddings,padding='max_length',truncation=True,return_tensors='pt')\n",
    "        label = self.tokenizer.encode(summary,max_length=256,padding='max_length',truncation=True,return_tensors='pt')\n",
    "        input_ids = inputs.input_ids\n",
    "        attention_mask=inputs.attention_mask\n",
    "\n",
    "        return dict(\n",
    "            input_ids =input_ids.squeeze(0),\n",
    "            attention_mask=attention_mask.squeeze(0),\n",
    "            label=label.squeeze(0),\n",
    "            text=text,\n",
    "            summary=summary\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262b513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartEncoder were not initialized from the model checkpoint at facebook/bart-large-cnn and are newly initialized: ['embed_positions.weight', 'embed_tokens.weight', 'layernorm_embedding.bias', 'layernorm_embedding.weight', 'layers.0.fc1.bias', 'layers.0.fc1.weight', 'layers.0.fc2.bias', 'layers.0.fc2.weight', 'layers.0.final_layer_norm.bias', 'layers.0.final_layer_norm.weight', 'layers.0.self_attn.k_proj.bias', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.out_proj.bias', 'layers.0.self_attn.out_proj.weight', 'layers.0.self_attn.q_proj.bias', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.bias', 'layers.0.self_attn.v_proj.weight', 'layers.0.self_attn_layer_norm.bias', 'layers.0.self_attn_layer_norm.weight', 'layers.1.fc1.bias', 'layers.1.fc1.weight', 'layers.1.fc2.bias', 'layers.1.fc2.weight', 'layers.1.final_layer_norm.bias', 'layers.1.final_layer_norm.weight', 'layers.1.self_attn.k_proj.bias', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.out_proj.bias', 'layers.1.self_attn.out_proj.weight', 'layers.1.self_attn.q_proj.bias', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.bias', 'layers.1.self_attn.v_proj.weight', 'layers.1.self_attn_layer_norm.bias', 'layers.1.self_attn_layer_norm.weight', 'layers.10.fc1.bias', 'layers.10.fc1.weight', 'layers.10.fc2.bias', 'layers.10.fc2.weight', 'layers.10.final_layer_norm.bias', 'layers.10.final_layer_norm.weight', 'layers.10.self_attn.k_proj.bias', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.out_proj.bias', 'layers.10.self_attn.out_proj.weight', 'layers.10.self_attn.q_proj.bias', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.bias', 'layers.10.self_attn.v_proj.weight', 'layers.10.self_attn_layer_norm.bias', 'layers.10.self_attn_layer_norm.weight', 'layers.11.fc1.bias', 'layers.11.fc1.weight', 'layers.11.fc2.bias', 'layers.11.fc2.weight', 'layers.11.final_layer_norm.bias', 'layers.11.final_layer_norm.weight', 'layers.11.self_attn.k_proj.bias', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.out_proj.bias', 'layers.11.self_attn.out_proj.weight', 'layers.11.self_attn.q_proj.bias', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.bias', 'layers.11.self_attn.v_proj.weight', 'layers.11.self_attn_layer_norm.bias', 'layers.11.self_attn_layer_norm.weight', 'layers.2.fc1.bias', 'layers.2.fc1.weight', 'layers.2.fc2.bias', 'layers.2.fc2.weight', 'layers.2.final_layer_norm.bias', 'layers.2.final_layer_norm.weight', 'layers.2.self_attn.k_proj.bias', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.out_proj.bias', 'layers.2.self_attn.out_proj.weight', 'layers.2.self_attn.q_proj.bias', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.bias', 'layers.2.self_attn.v_proj.weight', 'layers.2.self_attn_layer_norm.bias', 'layers.2.self_attn_layer_norm.weight', 'layers.3.fc1.bias', 'layers.3.fc1.weight', 'layers.3.fc2.bias', 'layers.3.fc2.weight', 'layers.3.final_layer_norm.bias', 'layers.3.final_layer_norm.weight', 'layers.3.self_attn.k_proj.bias', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.out_proj.bias', 'layers.3.self_attn.out_proj.weight', 'layers.3.self_attn.q_proj.bias', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.bias', 'layers.3.self_attn.v_proj.weight', 'layers.3.self_attn_layer_norm.bias', 'layers.3.self_attn_layer_norm.weight', 'layers.4.fc1.bias', 'layers.4.fc1.weight', 'layers.4.fc2.bias', 'layers.4.fc2.weight', 'layers.4.final_layer_norm.bias', 'layers.4.final_layer_norm.weight', 'layers.4.self_attn.k_proj.bias', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.out_proj.bias', 'layers.4.self_attn.out_proj.weight', 'layers.4.self_attn.q_proj.bias', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.bias', 'layers.4.self_attn.v_proj.weight', 'layers.4.self_attn_layer_norm.bias', 'layers.4.self_attn_layer_norm.weight', 'layers.5.fc1.bias', 'layers.5.fc1.weight', 'layers.5.fc2.bias', 'layers.5.fc2.weight', 'layers.5.final_layer_norm.bias', 'layers.5.final_layer_norm.weight', 'layers.5.self_attn.k_proj.bias', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.out_proj.bias', 'layers.5.self_attn.out_proj.weight', 'layers.5.self_attn.q_proj.bias', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.bias', 'layers.5.self_attn.v_proj.weight', 'layers.5.self_attn_layer_norm.bias', 'layers.5.self_attn_layer_norm.weight', 'layers.6.fc1.bias', 'layers.6.fc1.weight', 'layers.6.fc2.bias', 'layers.6.fc2.weight', 'layers.6.final_layer_norm.bias', 'layers.6.final_layer_norm.weight', 'layers.6.self_attn.k_proj.bias', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.out_proj.bias', 'layers.6.self_attn.out_proj.weight', 'layers.6.self_attn.q_proj.bias', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.bias', 'layers.6.self_attn.v_proj.weight', 'layers.6.self_attn_layer_norm.bias', 'layers.6.self_attn_layer_norm.weight', 'layers.7.fc1.bias', 'layers.7.fc1.weight', 'layers.7.fc2.bias', 'layers.7.fc2.weight', 'layers.7.final_layer_norm.bias', 'layers.7.final_layer_norm.weight', 'layers.7.self_attn.k_proj.bias', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.out_proj.bias', 'layers.7.self_attn.out_proj.weight', 'layers.7.self_attn.q_proj.bias', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.bias', 'layers.7.self_attn.v_proj.weight', 'layers.7.self_attn_layer_norm.bias', 'layers.7.self_attn_layer_norm.weight', 'layers.8.fc1.bias', 'layers.8.fc1.weight', 'layers.8.fc2.bias', 'layers.8.fc2.weight', 'layers.8.final_layer_norm.bias', 'layers.8.final_layer_norm.weight', 'layers.8.self_attn.k_proj.bias', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.out_proj.bias', 'layers.8.self_attn.out_proj.weight', 'layers.8.self_attn.q_proj.bias', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.bias', 'layers.8.self_attn.v_proj.weight', 'layers.8.self_attn_layer_norm.bias', 'layers.8.self_attn_layer_norm.weight', 'layers.9.fc1.bias', 'layers.9.fc1.weight', 'layers.9.fc2.bias', 'layers.9.fc2.weight', 'layers.9.final_layer_norm.bias', 'layers.9.final_layer_norm.weight', 'layers.9.self_attn.k_proj.bias', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.out_proj.bias', 'layers.9.self_attn.out_proj.weight', 'layers.9.self_attn.q_proj.bias', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.bias', 'layers.9.self_attn.v_proj.weight', 'layers.9.self_attn_layer_norm.bias', 'layers.9.self_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "cnn_large_model = BartEncoder.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "torch.save(cnn_large_model.state_dict(),'large_cnn.pt')\n",
    "lin_conf = BartConfig.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "lin_conf._attn_implementation=\"eager\"\n",
    "lin_model = LinformerEncoder(lin_conf).to(device)\n",
    "torch.save(lin_model.state_dict(),'linformer_cnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b63319cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_model.load_state_dict(torch.load('linformer_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658b5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for Total : 358900\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(dataset_pth,\"train.csv\")) \n",
    "train_ds = BartDataset(train_df,tokenizer,lin_conf)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds,batch_size=bsz,shuffle=True)\n",
    "total_steps = epoch*len(train_loader)\n",
    "print(f\"Training for Total : {total_steps}\")\n",
    "\n",
    "del train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b31ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(lin_model.parameters(),lr=learning_rate,weight_decay=weight_decay,eps=adam_eps)\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=warmup,num_training_steps=total_steps)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "# loss_fn = torch.nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5360766649246216\n",
      "1 0.5356727838516235\n",
      "2 0.5355427265167236\n",
      "3 0.5350778102874756\n",
      "4 0.5347800254821777\n",
      "5 0.5346702337265015\n",
      "6 0.5343059301376343\n",
      "7 0.5340139865875244\n",
      "8 0.5335469841957092\n",
      "9 0.5337418913841248\n",
      "10 0.53326416015625\n"
     ]
    }
   ],
   "source": [
    "layers=1\n",
    "for layer in range(layers):\n",
    "    total_loss=0\n",
    "    for ep in range(epoch):\n",
    "        lin_model.train(True)\n",
    "        loader = tqdm.tqdm(train_loader)\n",
    "        loader.set_description(f\"Layer : {layer} | Epoch : {ep}\")\n",
    "\n",
    "        for idx,data in enumerate(loader):\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            with torch.no_grad():\n",
    "                label_out = cnn_large_model(input_ids,attention_mask=None,output_hidden_states=True,output_attentions=True)\n",
    "            dist_inp = label_out.hidden_states[layer]\n",
    "            dist_out = label_out.hidden_states[layer + 1]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = lin_model(inputs_embeds=dist_inp,distil_layer=layer)\n",
    "\n",
    "            loss = loss_fn(dist_out,output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss+=loss.item()\n",
    "            loader.set_postfix({'Loss',loss.item()})\n",
    "\n",
    "        torch.save(dict(\n",
    "            model=lin_model.state_dict(),\n",
    "            loss=total_loss/len(train_loader),\n",
    "            epoch=ep,\n",
    "            layer=layer,\n",
    "\n",
    "        ),f'lin_model_dist_l{layer}.pt')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94776cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 1.8826e-02, -2.8632e-02, -1.4102e-02,  ..., -8.6234e-03,\n",
       "                       -6.2434e-02, -4.5047e-03],\n",
       "                      [ 2.2725e-03, -1.2984e-02, -1.0146e-02,  ..., -4.4377e-02,\n",
       "                       -1.1882e-03,  8.9085e-03],\n",
       "                      [-5.8181e-03, -3.8661e-02, -2.2369e-02,  ...,  5.7755e-03,\n",
       "                       -2.5276e-02, -2.0668e-03],\n",
       "                      ...,\n",
       "                      [ 4.7489e-02, -4.0045e-03,  4.5967e-03,  ..., -7.3304e-03,\n",
       "                       -1.7578e-03, -1.2105e-02],\n",
       "                      [-3.0868e-02,  9.6113e-03,  2.7596e-02,  ...,  9.0568e-05,\n",
       "                        6.7902e-03,  3.7463e-03],\n",
       "                      [ 7.6572e-03,  9.9940e-03, -2.5892e-02,  ...,  1.2484e-02,\n",
       "                       -1.8995e-02, -2.5772e-02]], device='cuda:0'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_model.layers[0].self_attn.kv_former.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3457d703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9033e-02, -2.8631e-02, -1.4096e-02,  ..., -8.8318e-03,\n",
       "         -6.2435e-02, -4.5005e-03],\n",
       "        [ 2.4507e-03, -1.2979e-02, -1.0137e-02,  ..., -4.4594e-02,\n",
       "         -1.1195e-03,  9.1001e-03],\n",
       "        [-5.8138e-03, -3.8660e-02, -2.2366e-02,  ...,  5.7585e-03,\n",
       "         -2.5275e-02, -2.0509e-03],\n",
       "        ...,\n",
       "        [ 4.7698e-02, -3.9829e-03,  4.7976e-03,  ..., -7.5373e-03,\n",
       "         -1.7084e-03, -1.2113e-02],\n",
       "        [-3.0878e-02,  9.8195e-03,  2.7811e-02,  ..., -5.9353e-05,\n",
       "          6.9939e-03,  3.9239e-03],\n",
       "        [ 7.8544e-03,  1.0202e-02, -2.5889e-02,  ...,  1.2477e-02,\n",
       "         -1.8992e-02, -2.5794e-02]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('linformer_cnn.pt')['layers.0.self_attn.kv_former.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b72d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
